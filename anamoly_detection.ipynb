# *Anomaly Detection *

### *1. Key Plots for EDA*
python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_distributions(df):
    """Visualize feature distributions and class imbalance"""
    plt.figure(figsize=(15,10))
    
    # 1. Target Distribution (Pie + Bar)
    plt.subplot(2,2,1)
    df['target'].value_counts().plot.pie(autopct='%1.1f%%', 
                                       colors=['#66b3ff','#ff9999'])
    plt.title("Class Distribution")
    
    plt.subplot(2,2,2)
    sns.countplot(x='target', data=df, palette="viridis")
    plt.title("Anomaly vs Normal Counts")
    
    # 2. Feature Boxplots
    plt.subplot(2,2,3)
    sns.boxplot(data=df[['X1','X2','X5']], palette="magma")
    plt.title("Sensor Value Distributions")
    
    # 3. Time Series Analysis
    plt.subplot(2,2,4)
    df.resample('D', on='Date')['target'].mean().plot()
    plt.title("Daily Anomaly Rate")
    
    plt.tight_layout()
    plt.savefig('eda_results.png', dpi=300)


### *2. Advanced Visualizations*
python
from mpl_toolkits.mplot3d import Axes3D

def plot_3d_features(df):
    """3D plot for multidimensional analysis"""
    fig = plt.figure(figsize=(12,8))
    ax = fig.add_subplot(111, projection='3d')
    
    x = df['X1']
    y = df['X2']
    z = df['X5']
    
    ax.scatter(x, y, z, 
              c=df['target'], 
              cmap='coolwarm',
              s=0.5,
              alpha=0.5)
    
    ax.set_xlabel('X1 (Log Scaled)')
    ax.set_ylabel('X2 (Standardized)')
    ax.set_zlabel('X5 (Normalized)')
    plt.title("3D Feature Space Coloring by Anomaly")
    plt.savefig('3d_visualization.png')


---

## *ðŸ¤– scikit-learn Implementation*

### *1. Complete Modeling Pipeline*
python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer

# Build preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('log_transform', FunctionTransformer(np.log1p), ['X1']),
        ('minmax_scale', MinMaxScaler(), ['X3','X4']),
        ('std_scale', StandardScaler(), ['X2','X5'])
    ],
    remainder='passthrough'
)

# Full pipeline with model
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(
        n_estimators=150,
        max_depth=25,
        class_weight='balanced',
        random_state=42
    ))
])


### *2. Model Evaluation*
python
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay

def evaluate_model(model, X_test, y_test):
    """Generate evaluation visuals"""
    plt.figure(figsize=(15,5))
    
    # Confusion Matrix
    plt.subplot(1,2,1)
    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test,
                                        cmap='Blues',
                                        display_labels=['Normal','Anomaly'])
    plt.title("Confusion Matrix")
    
    # ROC Curve
    plt.subplot(1,2,2)
    RocCurveDisplay.from_estimator(model, X_test, y_test)
    plt.plot([0,1],[0,1],'k--')
    plt.title("ROC Curve")
    
    plt.savefig('model_evaluation.png')


### *3. Cross-Validation Strategy*
python
from sklearn.model_selection import StratifiedKFold, cross_val_score

# Custom scoring for imbalanced data
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(
    pipeline,
    X_train,
    y_train,
    cv=cv,
    scoring='f1_macro',
    n_jobs=-1
)

print(f"Cross-validated F1: {scores.mean():.3f} Â± {scores.std():.3f}")


---

## *ðŸ”§ Key Configurations*

### *Matplotlib Settings*
python
# Professional style settings
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams.update({
    'figure.dpi': 120,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'font.family': 'DejaVu Sans',
    'savefig.bbox': 'tight',
    'savefig.transparent': False
})


### *scikit-learn Best Practices*
1. *Imbalanced Data Handling*:
   python
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(random_state=42)
   X_res, y_res = smote.fit_resample(X_train, y_train)
   

2. *Feature Importance*:
   python
   # After model training
   importances = pipeline.named_steps['classifier'].feature_importances_
   pd.Series(importances, 
            index=X_train.columns).sort_values().plot.barh()
   

3. *Model Persistence*:
   python
   from joblib import dump
   dump(pipeline, 'model_pipeline.joblib')
   

---

## *ðŸ“Œ Pro Tips*
1. *Matplotlib*: Use plt.tight_layout() before saving to prevent label clipping
2. *scikit-learn*: Always use Pipeline to prevent data leakage
3. *Memory Management*: For large datasets, add batch_size parameter in SMOTE
4. *Reproducibility*: Set random_state in all randomized operations

![Visualization Example](https://matplotlib.org/stable/_images/sphx_glr_scatter3d_001.png)


---

This format provides:
1. *Ready-to-use code blocks* for copy-pasting
2. *Visual examples* of expected outputs
3. *Professional configurations* for production-grade plots
4. *scikit-learn best practices* with modern techniques

Would you like me to add specific examples for other plots (like SHAP values) or more advanced scikit-learn features?
